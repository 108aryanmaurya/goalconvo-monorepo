# API Configuration (priority: OpenRouter > Groq > DeepSeek > Ollama > Gemini > OpenAI > Mistral)
# OpenRouter (priority 1 - set OPENROUTER_API_KEY to use; unified gateway, many models)
# OPENROUTER_API_KEY=your_openrouter_api_key_here
# OPENROUTER_API_BASE=https://openrouter.ai/api/v1
# OPENROUTER_MODEL=openai/gpt-3.5-turbo

# Groq (priority 2)
# GROQ_API_KEY=your_groq_api_key_here
# GROQ_API_BASE=https://api.groq.com/openai/v1
# GROQ_MODEL=llama-3.1-70b-versatile

MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_API_BASE=https://api.together.xyz/v1
MISTRAL_MODEL=mistralai/Mistral-7B-Instruct-v0.1

# Alternative API providers (uncomment to use)
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_API_BASE=https://api.openai.com/v1
# OPENAI_MODEL=gpt-3.5-turbo

# Generation settings
MAX_DIALOGUES=20000
BATCH_SIZE=10
TEMPERATURE=0.7
TOP_P=0.9
MAX_TOKENS=100

# Data paths
DATA_DIR=./data
SYNTHETIC_DIR=./data/synthetic
MULTIWOZ_DIR=./data/multiwoz
FEW_SHOT_HUB_DIR=./data/few_shot_hub

# Dialogue length (at least 6 turns; shorter turns = more concise)
MIN_TURNS=6
MAX_TURNS=15
MAX_TOKENS_USER_TURN=60
MAX_TOKENS_SUPPORTBOT_TURN=100

# Quality filtering
QUALITY_THRESHOLD=0.7
DISCARD_RATE=0.1

# Evaluation (comprehensive run after pipeline)
# Set to 1 to disable LLM-as-a-Judge and avoid extra API usage; GCR, TSR, diversity, etc. still run
# EVAL_SKIP_LLM_JUDGE=0

# Logging
LOG_LEVEL=INFO
LOG_FILE=./logs/goalconvo.log